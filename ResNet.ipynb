{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/architlahiri/all-resnet-implementations?scriptVersionId=141730109\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## ResNet Implementations\nIn this jupyter notebook I implement the Resent18, Resnet34 and Resnet50 architectures as proposed in the paper:\n\nHe et al., 2015. Deep residual networks for image recognition\nhttps://arxiv.org/pdf/1512.03385.pdf\n\nThe proposed models in the paper were created to fascilitate the training and use of very deep neural networks as the norm is that the deeper the neural net is, the more accurate it is as it is able to learn more complicated mathematical functions. \n\nThe issue the author's identified with regards to training of deep neural networks is that of vanishing/exploding gradients. In a neural network, gradient descent through back propogation works with gradient learning trickling down from deeper layers to lower layers. Which means that the learning value: learning_rate * (gradient) decreases the more backwards one goes through the network. In very large models, this means that the lower layers have either static gradients (vanishing) or exploding gradients as only few neurons out of many are activated continuously and can become extremely large and cause instability in training. These phenomenon are also called \"degradation\".\n\nTo solve these issues, the authors came up with the Residual Block. In this, there are skip connections which directly feeds the output of one layer to another layer which is deeper in the network- hence skipping some layers, like a shortcut. The residual function is the difference between the expected output and the input to a particular layer. It allows the network to focus on learning incremental changes (residuals) instead of attempting to learn the entire mapping from scratch, the training process becomes more manageable and convergence is improved.The residual function can easily learn the identiy mapping or get close. This means for backpropogation, lower layers are more easily influenced by changes in the deeper layers. This solves to a great degree the issue of degradation. \n\nThe authors showcase ResNets' capability to achieve significantly increased depth, up to 152 layers, outperforming VGG nets with lower complexity. Notably, an ensemble of these ResNets achieves a remarkable 3.57% error rate on the ImageNet test set, securing the top spot in the ILSVRC 2015 classification challenge. The paper also highlights the impact of deep representations, showcasing a substantial 28% relative improvement in COCO object detection. Their ResNet-based submissions dominate the ILSVRC & COCO 2015 competitions, achieving 1st place in various tasks including ImageNet detection, localization, COCO detection, and segmentation.\n\n","metadata":{}},{"cell_type":"markdown","source":"Here we import the necessary libraries. We are using the Tensorflow API.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport scipy.misc\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import random_uniform, HeNormal, constant, identity\nfrom tensorflow.keras.optimizers.legacy import SGD","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:08.962029Z","iopub.execute_input":"2023-08-31T14:31:08.962416Z","iopub.status.idle":"2023-08-31T14:31:08.969732Z","shell.execute_reply.started":"2023-08-31T14:31:08.962383Z","shell.execute_reply":"2023-08-31T14:31:08.968512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing\n\nI use the coffee beans dataset. It consists of 4 classes: Dark, Green, Light and Medium. The dataset is already divided into train (consisting of 1200 images: 300 per class) and test (consisting of 400 images: 100 per class). In the original paper, 224x224 crops are taken from the ImageNet data, however the coffee beans dataset is already at 224x224 size. In the paper the authors use per pixel mean subtraction for normalization however we divide the values by 255 as that is common practice right now. Furthermore the authors use a batch size of 256 but I use 32 as the dataset I use is much smaller. ","metadata":{}},{"cell_type":"code","source":"import pathlib\ndata_dir = pathlib.Path(\"/kaggle/input/coffee-bean-dataset-resized-224-x-224/train\")\nCLASS_NAMES = np.array([item.name for item in data_dir.glob('*')])\nprint(CLASS_NAMES)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:08.972199Z","iopub.execute_input":"2023-08-31T14:31:08.97272Z","iopub.status.idle":"2023-08-31T14:31:08.995434Z","shell.execute_reply.started":"2023-08-31T14:31:08.972686Z","shell.execute_reply":"2023-08-31T14:31:08.994483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32         \nIMG_HEIGHT = 224           \nIMG_WIDTH = 224            \n\nimage_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\ntrain_data_gen = image_generator.flow_from_directory(directory=str(\"/kaggle/input/coffee-bean-dataset-resized-224-x-224/train\"),\n                                                     batch_size=BATCH_SIZE,\n                                                     shuffle=True,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                     classes = list(CLASS_NAMES))\nvalid_data_gen = image_generator.flow_from_directory(directory=str(\"/kaggle/input/coffee-bean-dataset-resized-224-x-224/test\"),\n                                                     batch_size=BATCH_SIZE,\n                                                     shuffle=True,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                     classes = list(CLASS_NAMES))","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:08.997712Z","iopub.execute_input":"2023-08-31T14:31:08.99827Z","iopub.status.idle":"2023-08-31T14:31:09.060448Z","shell.execute_reply.started":"2023-08-31T14:31:08.998238Z","shell.execute_reply":"2023-08-31T14:31:09.059593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Skip Connections\n\nResnet18 and Resnet34 are different from Resnet50 and larger models in that the skip connections in Resnet18 and Resnet34 skip over two layer whereas in Resnet50 and above they skip over 3 layers. In this notebook we denote the skip connections that skip over 2 as Skip2 and those that skip over 3 as Skip3. \n\nThere are two kinds of residual blocks: Identity Blocks and Down-Sampling/Convolutional Blocks. \n\nThe identity block has two pathways. The main path ways consists of a series of convolutional and batch norm layers with \"same\" padding hence preserving the size/dimensions of the input into the block. The second pathway directly adds the initial input to the output of the first pathway, and because they are of same dimensions, they can be added directly. This is the residual block that can take the value of identity mappings. Relu activation is used in the first layer and after the two pathways have been added. \n\n","metadata":{}},{"cell_type":"code","source":"def Skip2_IdentityBlock(X,kernel_size,num_filters):\n    \n    X_shortcut = X\n    \n    X = Conv2D(kernel_size=kernel_size, filters=num_filters, strides=(1,1), padding='same')(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(kernel_size=kernel_size, filters=num_filters, strides=(1,1), padding='same')(X)\n    X = BatchNormalization(axis = 3)(X)\n      \n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:09.061773Z","iopub.execute_input":"2023-08-31T14:31:09.062082Z","iopub.status.idle":"2023-08-31T14:31:09.070874Z","shell.execute_reply.started":"2023-08-31T14:31:09.062051Z","shell.execute_reply":"2023-08-31T14:31:09.069986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The down-sampling block has two pathways. The main pathway consists of a series of convolutional and batch norm layers, however the first convolutional layer has a \"valid\" padding which means it reduces the dimensionality. To match this in the second pathway, the input is also passed through a convolutional layer with similar \"valid\" padding, kernel size and stride. These pathways are then added and the Relu activation function is applied onto these. This residual block cannot easily assume the form of identity mapping but performs the task of downsampling the image size which is seen in all convolutional neural networks.","metadata":{}},{"cell_type":"code","source":"def Skip2_DownSamplingBlock(X,kernel_size,num_filters,stride):\n    \n    X_shortcut = X\n    \n    X = Conv2D(kernel_size=kernel_size, filters=num_filters, strides=stride, padding='valid',kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(kernel_size=kernel_size, filters=num_filters, strides=(1,1), padding='same',kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    \n    X_shortcut = Conv2D(kernel_size=kernel_size, filters=num_filters, strides=stride, padding='valid',kernel_initializer = HeNormal())(X_shortcut)\n    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n\n    \n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:09.073278Z","iopub.execute_input":"2023-08-31T14:31:09.074462Z","iopub.status.idle":"2023-08-31T14:31:09.083877Z","shell.execute_reply.started":"2023-08-31T14:31:09.074297Z","shell.execute_reply":"2023-08-31T14:31:09.082971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resnet18 and Resnet34 are then implemented below. We pass in an image of size 224x224x3 for 3 colour channels RGB. We pad it to 3 pixels on all borders. This is as it is done in the original paper. With this, the dimension calculations of the convolutions will match the original paper. For Resnet18 and Resnet34, the Skip2 connections are used. The only difference between them is the number of such residual blocks. The first and final stages are same for both. The input is convolved, passed through a batch norm layer, activated using Relu and MaxPooling is applied to the result. The hyperparameters of all the layers are as described in the original paper. ","metadata":{}},{"cell_type":"code","source":"def ResNet18(num_classes, input_shape = (224, 224, 3), training=False): \n    \n    X_input = Input(input_shape)\n\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n    \n    #Stage 2\n    X = Skip2_IdentityBlock(X, 3, 64)\n    X = Skip2_IdentityBlock(X, 3, 64)\n    \n    #Stage 3\n    X = Skip2_DownSamplingBlock(X, 3, 128, 2)\n    X = Skip2_IdentityBlock(X, 3, 128)\n    \n    #Stage 4\n    X = Skip2_DownSamplingBlock(X, 3, 256, 2)\n    X = Skip2_IdentityBlock(X, 3, 256)\n    \n    #Stage 5\n    X = Skip2_DownSamplingBlock(X, 3, 512, 2)\n    X = Skip2_IdentityBlock(X, 3, 512)\n    \n    #Stage 6\n    X = AveragePooling2D()(X)\n    \n    X = Flatten()(X)\n    X = Dense(num_classes, activation='softmax', kernel_initializer = HeNormal())(X)\n\n    model = Model(inputs = X_input, outputs = X)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:09.085127Z","iopub.execute_input":"2023-08-31T14:31:09.086177Z","iopub.status.idle":"2023-08-31T14:31:09.099105Z","shell.execute_reply.started":"2023-08-31T14:31:09.086143Z","shell.execute_reply":"2023-08-31T14:31:09.098166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ResNet34(num_classes, input_shape = (224, 224, 3), training=False): \n    \n    X_input = Input(input_shape)\n\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n    \n    #Stage 2\n    X = Skip2_IdentityBlock(X, 3, 64)\n    X = Skip2_IdentityBlock(X, 3, 64)\n    X = Skip2_IdentityBlock(X, 3, 64)\n    \n    #Stage 3\n    X = Skip2_DownSamplingBlock(X, 3, 128, 2)\n    X = Skip2_IdentityBlock(X, 3, 128)\n    X = Skip2_IdentityBlock(X, 3, 128)\n    X = Skip2_IdentityBlock(X, 3, 128)\n    \n    #Stage 4\n    X = Skip2_DownSamplingBlock(X, 3, 256, 2)\n    X = Skip2_IdentityBlock(X, 3, 256)\n    X = Skip2_IdentityBlock(X, 3, 256)\n    X = Skip2_IdentityBlock(X, 3, 256)\n    X = Skip2_IdentityBlock(X, 3, 256)\n    X = Skip2_IdentityBlock(X, 3, 256)\n    \n    #Stage 5\n    X = Skip2_DownSamplingBlock(X, 3, 512, 2)\n    X = Skip2_IdentityBlock(X, 3, 512)\n    X = Skip2_IdentityBlock(X, 3, 512)\n    \n    #Stage 6\n    X = AveragePooling2D()(X)\n    \n    X = Flatten()(X)\n    X = Dense(num_classes, activation='softmax', kernel_initializer = HeNormal())(X)\n\n    model = Model(inputs = X_input, outputs = X)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:09.101931Z","iopub.execute_input":"2023-08-31T14:31:09.102526Z","iopub.status.idle":"2023-08-31T14:31:09.115979Z","shell.execute_reply.started":"2023-08-31T14:31:09.102493Z","shell.execute_reply":"2023-08-31T14:31:09.114736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We initialize the model to Resnet18. We could also change it to initialize to Resnet34 by using:\n\nmodel = ResNet34(input_shape = (224, 224, 3), num_classes = 4)","metadata":{}},{"cell_type":"code","source":"model = ResNet18(input_shape = (224, 224, 3), num_classes = 4)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:09.117843Z","iopub.execute_input":"2023-08-31T14:31:09.118174Z","iopub.status.idle":"2023-08-31T14:31:09.594636Z","shell.execute_reply.started":"2023-08-31T14:31:09.118143Z","shell.execute_reply":"2023-08-31T14:31:09.593698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As described in the paper, a momentum of 0.9 and weight decay of 0.0001 is used. The paper describes starting with the learning rate 0.1 and dividing it by 10 every time the error falls rapidly. We implement a standard learning rate of 0.001 as a more complex implementation of learning rate doesnt help us in the coffee beans dataset. We use accuracy as our metric and categorical cross entropy as our loss function.","metadata":{}},{"cell_type":"code","source":"optimizer = SGD(learning_rate=0.001, momentum=0.9, decay=0.0001)\n\nmodel.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:09.59613Z","iopub.execute_input":"2023-08-31T14:31:09.596535Z","iopub.status.idle":"2023-08-31T14:31:09.609123Z","shell.execute_reply.started":"2023-08-31T14:31:09.596496Z","shell.execute_reply":"2023-08-31T14:31:09.608179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_data_gen, validation_data=valid_data_gen, validation_freq=1, epochs = 20)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:31:09.610773Z","iopub.execute_input":"2023-08-31T14:31:09.611115Z","iopub.status.idle":"2023-08-31T14:33:22.685599Z","shell.execute_reply.started":"2023-08-31T14:31:09.611083Z","shell.execute_reply":"2023-08-31T14:33:22.684625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that over 20 epochs Resnet has learnt to predict the validation set extremely well :) Perhaps stopping it earlier would also give us better results. ","metadata":{}},{"cell_type":"markdown","source":"Below, we plot the accuracy history and we can visualize the learning of the model","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nf,ax=plt.subplots(1,1,figsize=(10,10)) \n\n#Plotting the training accuracy and validation accuracy\nax.plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\nax.plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:33:22.687132Z","iopub.execute_input":"2023-08-31T14:33:22.687552Z","iopub.status.idle":"2023-08-31T14:33:23.04318Z","shell.execute_reply.started":"2023-08-31T14:33:22.687516Z","shell.execute_reply":"2023-08-31T14:33:23.042235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resnet50 and beyond use skip3 connections. These involve one convolutional layer of kernel size 1, another of kernel size 3 and finally one of kernel size 1. There are also two types which can be called identity blocks and down-sampling blocks. The principle of working of these blocks is similar to the Skip2 connections. The only difference between Resnet50 and larger variants is the number of blocks which can be changed by anyone that wants to use this code for their own Resnet. ","metadata":{}},{"cell_type":"code","source":"def Skip3_IdentityBlock(X,kernel_size,initial_num_filters,final_num_filters):\n    \n    X_shortcut = X\n    \n    X = Conv2D(kernel_size=(1,1), filters=initial_num_filters, strides=(1,1), padding='valid', kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(kernel_size=kernel_size, filters=initial_num_filters, strides=(1,1), padding='same', kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(kernel_size=(1,1), filters=final_num_filters, strides=(1,1), padding='valid', kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    \n    # Adjust the dimensions of X_shortcut to match the number of final_num_filters\n    X_shortcut = Conv2D(kernel_size=(1, 1), filters=final_num_filters, strides=(1, 1), padding='valid', kernel_initializer = HeNormal())(X_shortcut)\n    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n    \n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:33:23.046666Z","iopub.execute_input":"2023-08-31T14:33:23.046969Z","iopub.status.idle":"2023-08-31T14:33:23.057407Z","shell.execute_reply.started":"2023-08-31T14:33:23.046936Z","shell.execute_reply":"2023-08-31T14:33:23.055078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Skip3_DownSamplingBlock(X,kernel_size,initial_num_filters,final_num_filters,stride):\n    \n    X_shortcut = X\n    \n    X = Conv2D(kernel_size=(1,1), filters=initial_num_filters, strides=stride, padding='valid', kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(kernel_size=kernel_size, filters=initial_num_filters, strides=(1,1), padding='same',kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(kernel_size=(1,1), filters=final_num_filters, strides=(1,1), padding='valid', kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    \n    X_shortcut = Conv2D(kernel_size=(1,1), filters=final_num_filters, strides=stride, padding='valid',kernel_initializer = HeNormal())(X_shortcut)\n    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n    \n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:33:23.059024Z","iopub.execute_input":"2023-08-31T14:33:23.059379Z","iopub.status.idle":"2023-08-31T14:33:23.072519Z","shell.execute_reply.started":"2023-08-31T14:33:23.059345Z","shell.execute_reply":"2023-08-31T14:33:23.071455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ResNet50(num_classes, input_shape = (224, 224, 3), training=False):\n    \n    X_input = Input(input_shape)\n\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = HeNormal())(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n    \n    #Stage 2\n    X = Skip3_IdentityBlock(X, 3, 64, 256)\n    X = Skip3_IdentityBlock(X, 3, 64, 256)\n    X = Skip3_IdentityBlock(X, 3, 64, 256)\n    \n    #Stage 3\n    X = Skip3_DownSamplingBlock(X, 3, 128, 152, 2)\n    X = Skip3_IdentityBlock(X, 3, 128, 152)\n    X = Skip3_IdentityBlock(X, 3, 128, 152)\n    X = Skip3_IdentityBlock(X, 3, 128, 152)\n    \n    #Stage 4\n    X = Skip3_DownSamplingBlock(X, 3, 256, 1024, 2)\n    X = Skip3_IdentityBlock(X, 3, 256, 1024)\n    X = Skip3_IdentityBlock(X, 3, 256, 1024)\n    X = Skip3_IdentityBlock(X, 3, 256, 1024)\n    X = Skip3_IdentityBlock(X, 3, 256, 1024)\n    X = Skip3_IdentityBlock(X, 3, 256, 1024)\n    \n    #Stage 5\n    X = Skip3_DownSamplingBlock(X, 3, 512, 2048, 2)\n    X = Skip3_IdentityBlock(X, 3, 512, 2048)\n    X = Skip3_IdentityBlock(X, 3, 512, 2048)\n    \n    #Stage 6\n    X = AveragePooling2D()(X)\n    \n    X = Flatten()(X)\n    X = Dense(num_classes, activation='softmax', kernel_initializer = HeNormal())(X)\n\n    model = Model(inputs = X_input, outputs = X)\n\n    return model\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:33:23.074631Z","iopub.execute_input":"2023-08-31T14:33:23.074982Z","iopub.status.idle":"2023-08-31T14:33:23.088163Z","shell.execute_reply.started":"2023-08-31T14:33:23.074949Z","shell.execute_reply":"2023-08-31T14:33:23.08719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we observe that Resnet50 works as expected.","metadata":{}},{"cell_type":"code","source":"model_resnet50 = ResNet50(input_shape = (224, 224, 3), num_classes = 4)\noptimizer = SGD(learning_rate=0.001, momentum=0.9, decay=0.0001)\nmodel_resnet50.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\nhistory_resnet50 = model_resnet50.fit(train_data_gen, validation_data=valid_data_gen, validation_freq=1, epochs = 20)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:33:23.089637Z","iopub.execute_input":"2023-08-31T14:33:23.090116Z","iopub.status.idle":"2023-08-31T14:37:25.143208Z","shell.execute_reply.started":"2023-08-31T14:33:23.090083Z","shell.execute_reply":"2023-08-31T14:37:25.142135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n#Plotting the training accuracy and validation accuracy\nax.plot(model_resnet50.history.history['accuracy'],color='b',label='Training  Accuracy')\nax.plot(model_resnet50.history.history['val_accuracy'],color='r',label='Validation Accuracy')\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:37:25.146556Z","iopub.execute_input":"2023-08-31T14:37:25.146898Z","iopub.status.idle":"2023-08-31T14:37:25.488617Z","shell.execute_reply.started":"2023-08-31T14:37:25.14687Z","shell.execute_reply":"2023-08-31T14:37:25.486935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for reading through the jupyter notebook! I hope you found it useful for your own applications. ","metadata":{}}]}